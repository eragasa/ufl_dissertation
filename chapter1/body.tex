\chapter{INTRODUCTION}\label{intro}

As compututational power has increased so has the size and complexity of the simulations.
The use of analytical empirical potentials has been part of computational materials science from the start, including problems with lattice dynamics, molecular dynamics, and other charges.
The accuracy of a simulation depends on many factors, some involving the simulation method itself (e.g. numerical accuracy in solving sets of equations).
Despite the promise of molecular dynamics, the difficulty in developing interatomic potentials leads to long developments when developing empirical potentials due to problems in determining an optimal parameterization.
This work presents an emergent framework for the automated development of potentials based upon sampling from a distribution and evolution of that distribution so that the final distribution represents the set of parameterizations which can be described in a way as efficient.
We present an novel framework for the automated development of potentials.

Our potential development approach takes a probabilistic approach, treating the vector of parameters as a random variable, where the variation in the values of the parameters represents the epistemic uncertainty associated with with the choice of parameter values.

However, the biggest errors in a simulation, as least with respect to how well it describes a real system, are the inadequacies of the models upon which the simulation is based.

\section{Machine Learning Frameworks}

In potential development,
Despite promising work with the development of machine learning potentials, which promises to provide \emph{ab initio} levels of fidelity, these approaches require large amounts of computational resources to development the requisite fitting datasets.
Machine learning potential development is a data hungry approach as to ensure that the problem is not functionally underdetermine due to the large number of degrees of freedom associated with the functional form.
Moreover, neural networks models themselves are NP-hard problems which maps the current configuration of the neural net onto potential configuration space, requiring an extraordinary computational effort to determine the most effective parameter optimization.
It should not be confused that neural networks are not an optimization solution, but actually an optimization problem, which are aided by different machine learning techniques which are dependent upon sampling

Instead, this work focuses on applying machine learning concepts to analytical interatomic potentials.  These models have a functional form described by equations which attempt to capture the relevant physics associated with a material system.  To adapt a functional form to a specific material system, a potential development process

\section{Modernizing Analytical Potential Development}

The modernization of analytical potential development needs to achieve the following goals: 
(1) clearly define the problem in its most general terms in what maybe, at times, a more rigorous mathematical exposition than that which is normally presented, 
(2) identify the problems with existing methodologies by bringing in terminology and notation from different fields to provide a more general framework for the problem of potential development, 
(3) provide a baseline implementation of this framework, which provides results which are analytical, transparent, and robust, and 
(4) provide an automation framework upon which to do the work.

\section{Objectives and Outline}
%% CHAPTER 2 SUMMARY
Chapter 2 provides an appropriate level of introduction to the major toolsets in computational materails science.
Starting with quantum mechanical approaches to solutions to the potential energy surface, a function which maps the configuration space of atomic descriptors onto energies.
Particular focus is placed on density functional theory (DFT), which is often used as the computational calculator to calculate difficult to observe experimentally.
From here, we look to two tools where analytical potentials are most often used:  molecular dynamic simulations and lattice dynamics simulations.  

%% CHAPTER 3 SUMMARY
Chapter 3 describes which to is referred to the the traditional approach to potential development.  Potential development is normally described as a quadratic programming optimization problem, applied to a description of a problem which it is not normally suited for.  However, this problem has never really been discussed clearly and explains why more recently many potential developers have moved from potential development

In the third chapter, begins with a discussion of the empirical interatomic potential is described as approximation as the potential energy surface.
The process of potential development starts with the selection of a fitting database, a finite set of material properties and the target values.
The traditional approach to potential development uses a scalar optimization approach by minimizing an objective function which is referred to in literature as the cost function.
The cost function is typically a weight sum of square differences between the predicted potential values and their respective target values.
This objective function in minimized usually through either constrained or unconstrained optimization techniques dependent upon a quadratic programming algorithm dependent upon first-order derivatives for optimization.  
As a result, quadratic programming techniques are likely unsuitable, except when an initial guess to the target parameterization is already known.
This problem is hinted at in discussions in literature in discussions in a move from local optimization vs global optimization algorithms.
In this chapter, we show that this method for even identifying the global minimum due to issues with (1) regularization, (2) necessary conditions for a solution are likely not satisfied (Karush-Kuhn-Tucker conditions), and at the very least impossible to prove.

To generalize the potential optimization process, we introduce the concept of multi-objective optimization (MOO) which is concerned with mathematical optimization problems involving more than one objective function to be minimized simultaneously.  Each material property is assigned a loss function, a measure of how good a prediction model does in terms of being able to predict an expected outcome.
In MOO approach to potential development, the choice of an optimal model choice must be aken in the presence of trade-offs between the choice of sacrificing the fidelity in predicting on material property at the expense of an another.
We accept that the development of an potential largely involves a decision of tradeoffs determined by the potential developer, these expression of preferences is inherently subjective.


We introduce the concept of a loss function as the primary mechanism for measuring the fidelity between the predictions of an empirical potential and the target reference value.
 and Pareto optimality as the more general problem of potential development.

However, more damning is that even when these conditions are met, the optimal paramaterization is dependent upon a vector of weights, which uniquely determines the parameterization.
That is, potential development is inherently a subjective process dependent upon the preferences of the user, which largely cannot be determine with certainty at the beginning of the process.

The ramification of chapter 3 is that potential optimization is NP-hard when preferences are fixed is that probing acceptable potentials by a mechanism of varying weights is computationally brutal, and reevaluates areas in parameter space which have already been solved.

%% CHAPTER 4 SUMMARY
Chapter 4 introduces a probability framework for representing the uncertainty associated with the parameterization.   After introducing an appropriate level of probability theory and notation, we connect the measure theoretic approach to probability to continuous probability distributions, generation of random variables, non-parametric probability distributions.  In addition, we review the Bayesian framework for parameter estimation, and adapt that framework to solve MOO problems.

Chapter 4 starts a framework for sampling in parameter space beginning with statistical concepts and links it to approaches in machine learning with genetic algorithms.

This work first starts with what is hoped an appropriate level of theory required to understand the necessities for automating the problem.
In much of the literature, there is rather extensive discussion of techniques and applications of techniques, but the supporting justification and rationalization of these techniques is necessarily short due to the format of publications.
Even when discussing older approaches, appropriate review articles have been identified from different fields into order to provide insight to the problem of potential development.


In the development of a solid methodology and a description of potential development automation, it is necessary first describe the major computational tools used within the framework of machine learning.  Specifically, the tools of Density Functional Theory, Molecular Dynamics, and Lattice Dynamics are briefly discussed to provide a sense of what is common and what is different between these computational approaches.  This is covered in chapter 2.

%% CHAPTER 5 SUMMARY
Chapter 5 describes an evolutionary framework which combines the concepts from Chapter 3 and Chapter 4 as an optimization framework.  
A monte carlo approach is used for parallelization.
%% CHAPTER 6 SUMMMARY
Chapter 6 describes the implementation of the evolutionary framework in \verb|pypospack|.  Instead of developing a large monolithic application, which is difficult to extend, modify, and implement.  
\verb|pypospak|Pypospack is conceived largely as software library which defines structure property relationships, how the structure property relationships are calculated, the process management of molecular dynamic simulations, parallel sampling, and potential selection.
%% CHAPTER 7,8,9
The next three chapters provides application of this framework for three different types of material systems.  Chapter 7 goes through the development of a Buckingham style potential on a prototypical oxide, magnesium oxide.  Chapter 8 goes through the development of a Stillinger-Weber potential for silicon.  Finally, Chapter 9 provides results for the developmen of an embedded atom method (EAM) potential for Nickel.

%% CHAPTER 10

Chapter 7,8,9 provide application of the framework for three different types of material systems.
ather than being competitive, \emph{ab initio} computational techniques are often integrated into potential development

This work takes a contrary, but complimentary approach by addressing the concerns readily brought force by this new avenue of study.
The promise of machine learning potentials is provide \emph{ab initio} level accuracy at a fraction of the cost of \emph{ab initio} techniques.

It is the opinion of the author, that the development of analytical empirical potentials has largely been retarded by the lack of standard tools and analytical frameworks.
Despite the ubiquity of molecular dynamics
Instead of looking to develop the optimal parameterization which is expected to replicate a large range of values.

