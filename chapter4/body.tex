\chapter{PROBABILITY METHODS}
\label{ch:probability}

Due to heavy use of simulation methodology involves in this work, a discussion of probability and simulation concepts to clarify terminology and notation.  What follows is a necessarily brief introduction to probability theory in a more rigorous sense, we refer the reader to classic textbooks of Rudin\cite{rudin1987_realanalysis} for a rigorous treatment of measure theory and Chung\cite{chung2001_probabilitytheory} for a measure theory construction of probability.

\section{Probability}
The discussion of the probability methods used in this work starts with the probability construction by Kolmogorov, which builds probability as a measure of sets, but adapted to the particular necessities of probability.

Let $(\Omega,\mathcal{F},\mathbb{P}$ be a measure space with $\mathbb{P}(\Omega)=1$.  Then $(\Omega,\mathcal{F},\mathbb{P}$ is a probability space, with sample space $\Omega$, event space $\mathcal{F}$, and probabilty measure $\mathbb{P}$.  The underlying foundation of any probability  distribution is the sample space, which is the set of all probable outcomes denoted as $\Omega$.  The realization of an outcome is denoted $\omega \in \Omega$.

The events for the measure space are defined in such a way that a probability measure can be assigned.  This allows to assign probability measures on complex events to characterize groups of outcomes.  The collection of all such events is a $\sigma$-algebra $\mathcal{F}$ of subsets of $\Omega$.  Not every subset of the sample space $\Omega$ must be considered an event, the $\sigma$-algebra restricts $\mathcal{F}$ to subsets of $\Omega$ for which $\mathbb{P}$ can be asssigned.

The probability measure, $\mathbb{P}$, a function which maps $\mathbb{P}:\mathcal{F}\rightarrow[0,1]$.  A probability is a real number between zero and one.  Within this work we do not disguish the difference between impossible events which have probability zero, and probability-zero events which are not necessarily impossible.  Events of probability one is an event that happens almost surely, with almost total certainty.

The triplet $(\Omega,\mathcal{F},\mathbb{P})$ is the probability measure space.

\subsection{Probability Measures for Continuous Variables}

\subsubsection{PDF}

\subsubsection{CDF}

\subsubsection{JPDF}

\section{Interpretation of Probabilities}

The meaing of probabilities assigned to potential values of a random variable is not of probability theory itself, but instead related to philosophical context over the interpretation of probability.  In most scientific application, the probability interpretation is physical in nature, in which probabilities are objective and an event's probability is the limit of its relative frequency as the number of samples becomes large.  This interpretation supports the statistical needs and inference requirements for experimental scientists since probabilities can be found by a repeatable objective process (e.g. experiments), and are thus devoid of opinion.  Within this context, probabilities are \emph{aleatory}.  Uncertainty is irreducible and inherent to to the physical process observed.

For finite temperature simulation above $0$ K, molecular dynamics simulations allow sampling from themodynamic ensembles.  Although the simulations are replicable since time-integration and the empirical potential are deterministic.  Sampling from the time-series or by randomly assigning initial velocities, allows molecular dynamics to model aleatory uncertainty associated with a model.

Within this work, probabilities are evidentiary and can be assigned to any problem statement, even when no random process is involved, as a way to represent subjective plausibility, or to the degree in which the statement is supported by available evidence.  Evidential probabilities here are considered to be degrees of belief, such as the Laplace interpretation where probabilities are defined in disposition to gamble at certain odds\cite{laplace_probability}.  Here, conherent subjective belief systems follow the laws of probability\cite{ramsey_probability,definetti_probability}.  Here, uncertainty is \emph{epistemic} and is based on current evidence, but is mutable based upon receiving new observations\cite{ramsey2016truth,definetti1980_foresight,jaynes2003_probability}.  This philsophical interpretation is more closely associatiated with Bayesian inference.

In practice, the uncertainty associated with any problem has both \emph{aleatory} and \emph{epistemic} components.  Within this context, the field of uncertainty quantification\cite{oberkamph} in involved the quantitative characterization and reduction of uncertainties in both computational and real world applications.  The difficulty of applying Bayesian frameworks to potential development is the evidence is constrained to the size of the dataset, a parameterized potential likely produced biases due to the inevitable tradeoffs in assigning preferences in predicting various material properties.  Moreover, the inability to define the uncertainities associated target properties frustrates standard VVUQ and Bayesian approaches.

 Neither of these interpretation is particularly useful for our application.  Here, the probability distribution should be interpreted as propensities.  In the propensity theory of probability, is the probability of an event to produce an outcome of a certain kind, or to yield a long-run relative frequency of such an outcome.  Propensities are not relative frequencies, but the purported causes of the observed stable relative frequencies.

Regardless the choice in the interpretation of the meaning of probabilities, the mathematics works the same.

Formally, a continuous random variable is a random variable whose cumulative distribution function (CDF) is continuous everywhere\cite{bertsekas2002_probabilitytheory}.  More practically, random variables of interest can be defined by a probability density function (PDF), which characterize the CDF.

\section{Random Variable}
A random variable $X$ is a variable whose possible values are outcomes of a random phenemon.  As a function, a random variable is required to be measurable, which rules out pathological issues.  A random variable has a probability distribution, which specificies the probability falls in.  Specifically, $X:\Omega\rightarrow\mathbb{R}$.  If a random variable $X:\Omega \rightarrow \mathbb{R}$ is defined on the probability space $(\Omega,\mathcal{F},\mathbb{P})$.  Then the probability of an event $A$ occuring is $\{\omega:X(\omega)=A\}$ which is denoted as $\mathbb{P}(X=A)$.

Formally, a random variable is particular type of measurable function.  However, there is an important difference which maybe more philosophical than mathematical due to the difference in the underlying domains.  A random variable operates on a set of outcomes or processes defined by $\Omega$, while deterministic variables does not.  This isn't a mathematical difference as the underlying domains are just sets, and the $\sigma$-algebra and measures provide the relevant mathematical structure.

For a probability space, even if a specific random process isn't mentioned, there is an implied assumption that such a random process exists.  Otherwise, there would be no need for probability to have its own language.

A random variable can only be evaluated by performing an unpredictable experiment.  That is, you don't choose which $\omega \in \Omega$ to choose.  If you did choose a specific $\omega$, the you aren't evaluating a random variable, you're just evaluating a measurable function.

\subsection{Expectation of a Random Variable}

If $X$ is a random variable with a finite number of outcomes, $\{x_1,x_2,...,x_k\}$ occurs with the probabilities $\{p_1,p_2,...p_k\}$.  Then the expectation of $X$ is defined as
\begin{equation}
  \mathbb{E}[X]=\sum_{i=1}^{k}x_i p_i
\end{equation}
If $X$ is a continuous random variable, then
\begin{equation}
  \mathbb{E}[X]=\int_{\Omega} X(\omega) d\mathbb{P}(\omega)
\end{equation}
If $X$ admits a density $f(x)$, then the expected value is defined as
\begin{equation}
  \mathbb{E}[X]=\int_{\mathbb{R}}x f_{X}(x) dx
\end{equation}

\subsection{Functions of Random Variables}
A new random variable can be defined by applying a measurable function to the outcomes of a a real-random variable $X$.  That is $Y=f(X)$.  Then the cumulative distribution of $Y$ can be defined as $F_Y(y)$
A probability density function for a random variable $X$ has a density $f_X$, where $f_X$ is a non-negative function:
\begin{equation}
  \mathbb{P}[a \leq X \leq b]=\int_{a}^{b}f_{X}(x)dx
\end{equation}

\subsection{Sampling a Random Variable}

In this section, we discuss the general concept of sampling using the technique of inverse transform sampling as a method for pseudo random number generation.  Inverse sampling samples from a uniform distribution, $u_i=U(0,1)$, and interprets it as a probability, and then returns the largest number x from the domain of the distribution $\mathbb{P}(X)$ such that $\mathbb{P}(-\infty<X<x) \leq u_i$.

Computationally, this method involves computing the CDF and inverting the function, which is known as the quantile function.

Note that for a discrete distribution, computing the CDF is not in general too difficult: we simply add up the individual probabilities for the various points of the distribution.  For a continuous distribution, however, we need to integrate the probability density function (PDF) of the distribution, which is impossible to do analytically for most distributions (including the normal distribution).

\subsection{Multivariate Sampling}
\label{section:multivariate_sampling}

\section{Estimating Probability Distribution Functions}

\subsection{Parameteric Distributions}

\subsection{Non-parameteric Distributions}


\subsection{Bayesian Inference}
\label{section:bayes}

Let $\hat{\bm{X}}$ be a sample population with the observed data points, $\hat{x}_i \in \hat{\bm{X}}$.

The \emph{prior distribution} is the probability distribution before any data is observed.  If $\bm{\theta}$ is a vector of parameters for the distribution of $X$.   Since this distribution is dependent upon \emph{a priori} information, the prior distribution might not be easily determined.  In this case, a non-informative distribution can be used to as an initial estimate, which is updated based upon newer observations.

The \emph{sampling distribution} is the distribution of the observed data conditional on its parameters, $f(\bm{X})$

\subsection{Uninformative Distributions}
\label{section:bayes_prior}
A prior proabability distribution of a certain  quantity is the probability distribution that would express one's beliefs about this quantity before some evidence is taken into consideration.

Since the posterior density $f_{post}$ is proportional to $L(x)f_{prior}$, the maximimum likelihood will match the maximum posterior if the maximum of the likelihood is a maximum of the prior distribution.  Since a uniform prior means that the prior is constant, we get that the posterior is proportional to the likelihood, so they have the same maxima.

An in-depth discussion about the choice of uniform priors are

\section{Density Estimation}

\subsection{Parametric Estimation}

If $X$ is a random variable, then a parametric model for the probability distribution function of $x \in X$ implements are restricted set of functions, $f(x;w)$, where $w$ are the parameters of the probability distribution function $p(x)=f(x;w)$.

This work only concerns itself with with two parametric probability distribution functions: the uniform distribution and the multivariate distribution function.  The uniform distribution is defined by the lower bound $a$ and the upper bound $b$ of the probability distribution function, which is constrained by its use to define an uninformative prior distribution to incorporate \emph{a priori} information.

Maximum Likelihood Estimator

The second probability distribution is the Normal distribution, where the MLE estimators for is the meand and variance.

The advantages of parameteric distributions are (1) dependence of the function $f$ on a few parameters makes density estimation from well-known MLE estimation straightfoward, (2) the models are compact which makes them memory efficient and less demanding computationally, (3) since parametric distributiosn make strong \emph{a priori} assumptions about the underlying distributions, they deliver excellent results if these assumptions are not violated.  The strong \emph{a priori} assumptions makes it difficult to express ignorance when an inappropriate model is chosen to model data which violates those assumptions.

\subsection{Non-parametric Estimation}
Non-parametric methods on the other hand make only weak, general prior assumptions about the data, such as smoothness, $f(x;w)$ is constructed over the the training data $X$, the construction involves no or few parameters to be either estimated or "learned".

few or no parameters to fit => “learning” is easy
very flexible: can fit (almost) any data well
requires virtually no prior knowledge

expensive in memory and CPU (must store all data)
not much opportunity to incorporate prior knowledge

Expectation Maximization

\subsection{Kernel Density Estimation}
%% https://nic.schraudolph.org/teach/ml03/ML_Class4.pdf
\subsubsection{}
Let $(x_1,x_2,...,x_n)$ be a univariate and identically distributed sample drawn from some distribution with an unknown density $f$.
The goal is to estimate the shape of this function $f$.  The kernel density estimator is
\begin{equation}
  \hat{f}_h(x)=\frac{1}{n}\sum_{i=1}^{n}K_h(x-x_i)
    =\frac{1}{nh}\sum_{i=1}^{n}K\left(\frac{x-x_i}{h}\right)
\end{equation}
where $K$ is the kernel and $h>0$ is a smoothing parameter called the badwidth.  The kernel function satisfies the condition
\begin{equation}
  \int_{-\infty}^{+\infty}K(x)dx=1
\end{equation}
\subsubsection{Choice of kernels}
Popular kernels: Epanachnikov, Bi-weight, Triangular, Gaussian, Rectangular.

For the kernel method, we adopt the gaussian kernel $\phi$,
\begin{equation}
  \phi(x)=\frac{1}{\sqrt{2\pi\sigma}}\exp{\left(\frac{t^2}{2\sigma^2}\right)}
\end{equation}
\subsubsection{Selection of bandwidth parameters}
The chosen bandwith is important because it has a strong influenjce on the boundary of the density curve.  The curve boundary has poor smoothness quality when bandwidth takes small value; while as the increasing bandwidth, the smoothness improves, but the fitness of the curves becomes poor.  The accuracy of kernel estimation is dependent on suitable bandwidth.

\emph{Scott' Method}

\emph{Silverman Method}

\emph{Chiu Method}

\section{Mixture Models}

\section{Bayesian Estimation}
The essential difference between Bayesian and frequentist approaches to probability is how probability is used.  In a Frequentist arppoach, probability is used to model processes from observed samples.  In this case, all uncertainty is treated as \emph{aleatory}.  Aleatory uncertainty is irreducible, in that there will always be variability in the underlying variables.  For example, flipping a coin and predicting "heads" or "tails" is aleatory uncertainty.  The uncertainty we are observing is random, it is part of the natural process of what we are observing.

Bayesian approaches allows modelling not only of \emph{aleatory} uncertainty, but \emph{epistemic} uncertainty.  Epistemic uncertainty is reflects the limited knowledge we have on a system.   This type of uncertainty of reducible.

\section{Kullbach Leiber Divergence}

\subsection{AIC vs BIC}

\section{Dimensionality Reduction Methods}

\section{Sampling}

Requirements for kernel
\begin{table}[htbp]
   \caption{Sample size required to ensure relative mean square error at zero is
       less than $0.1$, when estimating a standard normal density using a normal
       kernel and the window width that minimize the mean square error loss at
       zero\cite{silverman1986_density_estimation}}
   \label{tab:kde_sample_req}
   \begin{tabularx}{6.5in}{XX}
     \hline
     Dimenstionality & Required Sample Size \\
     \hline
     1 & 4 \\
     2 & 19 \\
     3 & 67 \\
     4 & 223 \\
     5 & 768 \\
     6 & 2790 \\
     7 & 10700 \\
     8 & 43700 \\
     9 & 187000 \\
     10 & 842000 \\
     \hline
   \end{tabularx}
\end{table}

\section{Monte Carlo Methods}

Monte Carlo methods are a broad class of computational algorithms which are dependent upon random sampling to obtain numerical results.
The essential aspect of these approaches is to use randomness to solve problems which might be deterministic in principle.
Monte Carlo simulations sample from a probability distribution for each variable to produce an arbitrarily large number of possible outcomes, and  the results are analyzed to get probabilities of different outcomes occuring.

\section{Normal Distribution}

Let $X$ be a random variable defined by a multivariate random variable defined by $\bm{\mu}$

\section{Kernel Density Estimate}

\section{Gaussian Mixture Models}
%% https://brilliant.org/wiki/gaussian-mixture-model/

Gaussian mixture models (GMM) are a probabilistic model for representing normally distributed subpopulations within an overall population. Mixture models in general don't require knowing which subpopulation a data point belongs to, allowing the model to learn the subpopulations automatically. Since subpopulation assignment is not known, this constitutes a form of unsupervised learning.

A Gaussian mixture model is parameterized by two types of values, the mixture component weights and the component means and covariances.  For a Gaussian mixture model with $K$ components, the $k$th component has a a mean $\mu_k$ and $\sigma_k$ for the univariate case.

For the $d$-dimensional multivariate case, these are replaced by the vector $\bm{\mu}_k \in \mathbb{R}^d$ and the variance-covariance matrix

If the number of components $K$ is known, expectation maximum

\section{Model Validation}

\subsection{Resampling}
%% https://nic.schraudolph.org/teach/ml03/ML_Class6.pdf
\subsubsection{Test Set Method}
\subsubsection{Cross Validation}
\subsection{Analytic}
\subsubsection{AIC}
\subsubsection{BIC}
