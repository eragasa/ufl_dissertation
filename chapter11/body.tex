\chapter{SUMMARY OF WORK}
\label{ch:summary}

\section{Significant Contributions}
This work set out to present an emergent framework for the automated development of analytical potentials.  Here we summarize key contributions in this work.

In Chapter \ref{ch:potential_development}, a critical assessment of the conventional process for potential paramaterization was provided.  We identified already known weakness with the process, but presented them in a more rigorous way.  Using cardinal optimization techniques is inappropriate because conditions for convergence to a global solutions are not met.  Convergence to global minima can only be met if the process is repeated across a dense range of initial conditions in space of possible potential parameterizations, or if random sampling techniques are employed.  In addition, the cost function formulation is unable to identify regions in the predictive performance when the optimal performance region is convex.  This creates high sensitivity to encoded preferences in convex regions due to the existence of degenerate solutions.  Finally, the \emph{a priori} expression of weights is required to be encoded at the beginning of the process.

We introduce the concept of ordinal minimization to the field of potential development, where the solution to parameter optimization is not expressed in an arbitrary cost function, which serves as an artifical heuristic, but in ordinal terms where we calculate an indifference curve based upon the concept of Pareto optimality where each candidate parameterization is mutally non-dominated.  This formulation elminates problems of local minima, expression of \emph{a priori} expression of weights, only imposes the condition the preservation ordinality to measure losses with respect to individual material properties of interest, and can identify solutions in convex regions of performance space normally occluded to cardinal optimization methods.

When the optimal performance has convexity or discontinuities, we show that the computational efficiency of Monte Carlo sampling techniquqes provides an order of magnitude improvement in  numerical efficiency in estimating the Pareto optimal surface, while simultaneously providing an order of magnitude computational efficiency in the number of simulations required.  In addition, ordinal optimization uses independent sampling which by definition provides near linear scalability in the ability of the algorithm to use processors.

Chapter \ref{ch:methodology} builds upon those concepts.  The implementation of constraints both in the domain of potential parameter design variable and the predictive performance is difficult within a cardinal optimization context, since Karush-Kuhn-Tucker conditions are likely not met, and most certaintly cannot be proved.  Cardinal optimization techniques have the undesirable task of implementing a non-linear programming scheme based upon simulations which must be treated as black box functions.  Here, we presented a modified Monte Carlo technique which uses acceptance-rejection sampling to modify the the distribution of the random variable \emph{ex post}.  This technique is considerably more flexible as ordinal optimization schemes are based upon criteria which are only dependent upon expressing an inequality.  This technique starts with the expression of the epistemic uncertainty that a potential developer has respect the location of the desireable potential parameters, and updates it in a Bayesian manner using evidence generated from Monte Carlo sampling.  This defines a new evolutionary algorithm for potential optimization that applicable for any multi-objective optimization program.

Chapter \ref{ch:software} implements these concepts into software.  A new software code \emph{pypospack} was developed to provide a framework for optimizing interatomic potentials.  Here the implementation of potentials, simulation tasks, and material properties are decomposed as independent components in an object-oriented architecture.  The use of inheritance allows us to encapsulate implementation details such as process handling, parallelization, and workflow management.  In turn, this lowers the necessary effort and expertise to contribute to the project.  An application \emph{pyposmat} which uses the \emph{pypospack} software libraries is used to implement the methodology described in Chapter \ref{ch:methodology}, and is applied to specific material systems in Chapters \ref{ch:ionic_MgO} and \ref{ch:pareto_si}.

Chapter \ref{ch:ionic_MgO} demonstrates that our unsupervised algorithm can deliver similar performance as expert-developed potentials for the magnesium oxide ionic system.  However, our Pareto optimization strategy has the inverse problem of cardinal optimizaton.  While cardinal optimization produces too few solutions.  The Pareto optimization strategy produces too many.  To solve this problem, we apply biobjective visualization techniques which projects the high dimensional data into a series of two-dimensional subspace.  This allows us to identify the performance tradeoffs in a global bivariate sense.

Chapter \ref{ch:pareto_si} introduces classification algorithms to the set of candidate potentials.  In the potential develop of a Stillinger-Weber potential for silicon, a large number of potential parameterizations are identified.  The number of potentials is so large that visualization techniques fail.  Here we apply the Gaussian mixture model (GMM) to partition the population and select a much smaller number of components determined by calculating the information criteria, which penalizes the selection of a more complicated model that adds too many components to the GMM.  This produces clusters which can be described with analytical multi-variate normal distributions.  The Student's $t$-test is used to assess the predictive performance of each material property of each cluster, and aggregate test for the performance was constructed.  This provides an algorithmic method for eliminating clusters with undesirable predictive based upon a $p$-value statistical tests.

\section{Future Work}

This new methodology is promising, but recommendations for future work and improvements are now discussed.

In order to provide continual improvement of the Pareto surface, it is necessary to constantly remove candidate potentials.  This process defines a cardinal scoring functions then ranks the potentials eliminating the poorest performance with respect to this metric.  Here, we scale the absolute errors by the target values and eliminate the worst performers by varying the volume of a hypersphere in this scaled space until a desired percentile of the candidate potentials are contained within this hypersphere.  Here, the choice of scaling is somewhat arbitrary and approximate, but serves to scale the errors to the same order of magnitude.  However, this filter seems to govern the optimization process and should be studied further.

This process produces are large number of candidate parameterizations.  However, the analysis space is high dimensional both in the design space of the potential parameters as well as the response space of predictive performance.  Linear projections into a two-dimensional subspace results in the loss of data.  The ill-conditioning of the covariance matrix suggests variance reduction techniques could be performed using eigenvalue-eigenvector analysis using a process such as principle components analysis, to project into a lower dimensional subspace without sacrificing too much information loss.

In addition, the distribution of parameters and predicted material properties are likely clustered due the existence both convex and concave regions of the Pareto surface.  We have demonstrated that clustering techniques are promising, but methodologies to translate these results into potential selection and improved sampling techniques have not been adequately explored.

On the software wide, we implement a simple parellization scheme which partitions the sample space and each processor processes an equal number of potentials.  However, some parameterizations may take more time to produce a result than others.  This leads to some processors finishing before others.  A more efficient concurrency scheme could be implemented.

Currently, the calculation of the Pareto surface and the calculation of the bandwidth parameter using cross-valdiation techniques are computational limitations since these calculations are performed on a single processor which increases memory requirements and computational time.   Parallelization schmes for both these processes would improve scalability.

Our potential produces a large amount of data.  Currently, the sampling scheme only uses information about the location of Pareto optimal points in determining random variates.  However, since the Pareto surface is the performance envelope between feasible and infeasible space, the dominated potentials provide considerable information on where to direct sample dispersions.  In addition, \emph{pypospack} produces a large amount of structured information stored in flat files.  The interactivity in analysis can be improved by integrating this information into a database, which can ease workstation analysis requirements due to the considerable memory and input-output demands of analysis.

Finally, the methodology provided here is specifed in probabilistic terms to make it compatible with VVUQ techniques.  This goal has not been accomplished due to the inadequacies of the formalism of the interatomic potential.  The reduction of epistemic uncertainty when performance tradeoffs are large implies a high degree of uncertainty in distribution of the potential parameters.  The modification of standard VVUQ techniques are necessary to deal with the inevitable biases when selecting a region of parameterization, particularly when dealing with propagating parametric uncertainty to dynamic simulations.
